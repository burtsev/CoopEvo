{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/burtsev/CoopEvo/blob/master/notebooks/RMT_GPT_Multi_Stream.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS3Sz9sUfuBK"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers\n",
        "! pip install wandb\n",
        "! git clone https://github.com/burtsev/RMT-experiments\n",
        "#%cd RMT-experiments\n",
        "#! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOW1CCCpfq6L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tqdm\n",
        "import torch\n",
        "import datasets\n",
        "import json\n",
        "import wandb\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "from itertools import chain\n",
        "from torch.utils.data import DataLoader#, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "sys.path.append('RMT-experiments')\n",
        "#sys.path.append('..')\n",
        "wandb.login(key='e7a6323eda0d0dfb427e61f332a5eb3b151c7bab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwdIjVjmfq6N"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJFsf5kR6-Lf"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ht7gtRg95DaX"
      },
      "outputs": [],
      "source": [
        "from base_models.modeling_gpt_neox_multi_str import GPTNeoXForCausalLM\n",
        "\n",
        "model_name = 'EleutherAI/pythia-70m-deduped'\n",
        "config_name = 'neox_6l4hd1024'\n",
        "config_path = '/content/RMT-experiments/base_models/configs/gptconfigs/' + config_name + '.json'\n",
        "with open(config_path, 'r') as file:\n",
        "    wb_cfg = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eWcuCEbigsQ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_cfg = AutoConfig.from_pretrained(config_path)\n",
        "model = GPTNeoXForCausalLM(config=model_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g1_I-8-fq6Q"
      },
      "outputs": [],
      "source": [
        "input_size = 512\n",
        "memory_size = 0\n",
        "n_segments = 1\n",
        "batch_size = 8\n",
        "\n",
        "block_size = input_size\n",
        "block_size -= 2 * memory_size\n",
        "history_size = (n_segments - 1) * block_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36JHPQf4fq6S"
      },
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMGdrkYjfq6S"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples, block_size, history_size=None):\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "    if history_size is None:\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "    else:\n",
        "        result = {\n",
        "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "def collate_fn(batch):\n",
        "    input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
        "    labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
        "    attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
        "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
        "    labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
        "    attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
        "\n",
        "    collated = {'input_ids': input_ids,\n",
        "                'labels': labels,\n",
        "                'attention_mask': attention_mask}\n",
        "\n",
        "    if input_ids.shape[1] != block_size:\n",
        "        labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
        "        labels_mask[:, :-block_size] = False\n",
        "        collated['labels_mask'] = labels_mask\n",
        "\n",
        "    return collated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebbBLzbKfq6T"
      },
      "outputs": [],
      "source": [
        "task_name = 'wikitext-103-v1' #'wikitext-2-v1'\n",
        "raw_datasets = datasets.load_dataset('wikitext', task_name)\n",
        "column_names = raw_datasets[\"train\"].column_names\n",
        "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[text_column_name])\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size),\n",
        "                                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
        "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size, history_size),\n",
        "                                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo7bwk6Jfq6U"
      },
      "outputs": [],
      "source": [
        "train_rnd_generator = torch.Generator()\n",
        "train_rnd_generator.manual_seed(42)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn,\n",
        "                                shuffle=True, drop_last=False, generator=train_rnd_generator, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                                        collate_fn=collate_fn, shuffle=False, drop_last=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3V2cW5pfNim"
      },
      "outputs": [],
      "source": [
        "# Create an iterator from the DataLoader\n",
        "gen = iter(train_dataloader)\n",
        "\n",
        "# Define the batch number you want to print\n",
        "target_batch_number = 111\n",
        "current_batch_number = 0\n",
        "\n",
        "# Iterate over the DataLoader\n",
        "for batch in gen:\n",
        "    if 'labels_mask' in batch:\n",
        "        batch.pop('labels_mask')\n",
        "    if current_batch_number == target_batch_number:\n",
        "        # Move the batch to the device (e.g., CPU or GPU)\n",
        "        for k, v in batch.items():\n",
        "            batch[k] = v.to(device)\n",
        "\n",
        "        # Print the content of the specific batch\n",
        "        print(f\"Content of Batch {current_batch_number}:\")\n",
        "        for key, value in batch.items():\n",
        "            print(f\"\\n{key}:\")\n",
        "            if key == 'labels' and isinstance(value, torch.Tensor):\n",
        "                # Decode each sequence in the tensor\n",
        "                for i, seq in enumerate(value):\n",
        "                    decoded_seq = tokenizer.decode(seq, skip_special_tokens=True)\n",
        "                    print(f\"Decoded Sequence {i} in {key}: {decoded_seq}\")\n",
        "            elif isinstance(value, torch.Tensor):\n",
        "                print(f\"{key} Tensor: {value}\")\n",
        "            else:\n",
        "                print(\"[Not a tensor]\", value)\n",
        "\n",
        "        # Check and print 'labels_mask' if it exists\n",
        "        if 'labels_mask' in batch:\n",
        "            print(\"\\nlabels_mask:\")\n",
        "            print(batch['labels_mask'])\n",
        "        else:\n",
        "          print('No labels_mask')\n",
        "\n",
        "        # Stop after finding and printing the desired batch\n",
        "        break\n",
        "\n",
        "    # Increment the batch number\n",
        "    current_batch_number += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IctuVyglfq6V"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nildVryzfq6V"
      },
      "outputs": [],
      "source": [
        "gen = iter(train_dataloader)\n",
        "batch = next(gen)\n",
        "if 'labels_mask' in batch:\n",
        "    batch.pop('labels_mask')\n",
        "for k, v in batch.items():\n",
        "    batch[k] = v.to(device)\n",
        "batch['input_ids'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tLe9M1cfq6W"
      },
      "source": [
        "### Add RMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uddGTZ1_fq6X"
      },
      "outputs": [],
      "source": [
        "from modeling_rmt.language_modeling import MemoryCell, RecurrentWrapper\n",
        "\n",
        "cell = MemoryCell(model, num_mem_tokens=memory_size)\n",
        "model = RecurrentWrapper(cell,\n",
        "                        segment_size=block_size,\n",
        "                        max_n_segments=n_segments,\n",
        "                        )\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuMOLNOMfq6X"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    out = model(**batch)\n",
        "    print('Success!')\n",
        "except IndexError:\n",
        "    print('Error: Input size too large!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmD1a13zfq6X"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp1BwBb7fq6Y"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "learning_rate = 1e-04\n",
        "optim = AdamW(params=model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLvmUQ_yfq6Y"
      },
      "outputs": [],
      "source": [
        "train_steps = 20000\n",
        "eval_steps = 50\n",
        "\n",
        "train_gen = iter(train_dataloader)\n",
        "valid_gen = iter(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j1UjijcZVA9"
      },
      "outputs": [],
      "source": [
        "run_cfg = {\n",
        "    'input_size': block_size,\n",
        "    'memory_size': memory_size,\n",
        "    'n_segments': n_segments,\n",
        "    'batch_size': batch_size,\n",
        "    'model_name': model_name,\n",
        "    'config_name': config_name,\n",
        "    'learning rate': learning_rate,\n",
        "}\n",
        "wb_cfg.update(run_cfg)\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"RMT GPT\",\n",
        "    name='mem'+str(memory_size)+'_inlen'+str(block_size)+'_seg'+str(n_segments)+'_Multi_Str_S0S2D2_'+config_name+'_'+task_name,\n",
        "    config=wb_cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4oBwkXq7fq6Y"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "# Initialize the progress bar\n",
        "progress_bar = tqdm.notebook.tqdm(range(train_steps), desc='Training Progress')\n",
        "\n",
        "# Create an iterator from the DataLoader\n",
        "train_iterator = iter(train_dataloader)\n",
        "\n",
        "for step in progress_bar:\n",
        "    optim.zero_grad()\n",
        "\n",
        "    try:\n",
        "        batch = next(train_iterator)\n",
        "    except StopIteration:\n",
        "        # Reset the iterator when the end of the dataset is reached\n",
        "        train_iterator = iter(train_dataloader)\n",
        "        batch = next(train_iterator)\n",
        "\n",
        "    # Move the batch to the device\n",
        "    for k, v in batch.items():\n",
        "        batch[k] = v.to(device)\n",
        "\n",
        "    out = model(**batch)\n",
        "    loss = out.loss\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if step % eval_steps == 0:\n",
        "        losses.append(loss.detach().item())\n",
        "        current_loss = loss.item()\n",
        "        progress_bar.set_description(f\"Step {step}/{train_steps} - Loss: {current_loss:.4f}\")\n",
        "        wandb.log({'step': step, 'loss': current_loss})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpu--EUxh06z"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses, label='Baseline 0 mem')\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('train loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "floJ3ftSsKYY"
      },
      "outputs": [],
      "source": [
        "loss_base = losses\n",
        "print(memory_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAqzlHvRkrMZ"
      },
      "source": [
        "# Mem run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-taCe7Jkkvh"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_cfg = AutoConfig.from_pretrained(config_path)\n",
        "model = GPTNeoXForCausalLM(config=model_cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJyAjCvekkvr"
      },
      "outputs": [],
      "source": [
        "input_size = 4\n",
        "memory_size = 2\n",
        "#n_segments = 2\n",
        "#batch_size = 32\n",
        "\n",
        "block_size = input_size\n",
        "block_size -= 2 * memory_size\n",
        "history_size = (n_segments - 1) * block_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvY2awMLkkvt"
      },
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0RX6Ilykkvv"
      },
      "outputs": [],
      "source": [
        "def group_texts(examples, block_size, history_size=None):\n",
        "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "    if history_size is None:\n",
        "        result = {\n",
        "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "    else:\n",
        "        result = {\n",
        "            k: [t[max({0, i - history_size}) : i + block_size] for i in range(0, total_length, block_size)]\n",
        "            for k, t in concatenated_examples.items()\n",
        "        }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "id_pad_value = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "def collate_fn(batch):\n",
        "    input_ids = [torch.tensor(b['input_ids'][::-1]) for b in batch]\n",
        "    labels = [torch.tensor(b['labels'][::-1]) for b in batch]\n",
        "    attention_mask = [torch.tensor(b['attention_mask'][::-1]) for b in batch]\n",
        "    input_ids = pad_sequence(input_ids, padding_value=id_pad_value).T.flip(1)\n",
        "    labels = pad_sequence(labels, padding_value=-100).T.flip(1)\n",
        "    attention_mask = pad_sequence(attention_mask, padding_value=0).T.flip(1)\n",
        "\n",
        "    collated = {'input_ids': input_ids,\n",
        "                'labels': labels,\n",
        "                'attention_mask': attention_mask}\n",
        "\n",
        "    if input_ids.shape[1] != block_size:\n",
        "        labels_mask = torch.ones_like(input_ids, dtype=bool)\n",
        "        labels_mask[:, :-block_size] = False\n",
        "        collated['labels_mask'] = labels_mask\n",
        "\n",
        "    return collated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h746fy_Fkkvy"
      },
      "outputs": [],
      "source": [
        "task_name = 'wikitext-2-v1'\n",
        "raw_datasets = datasets.load_dataset('wikitext', task_name)\n",
        "column_names = raw_datasets[\"train\"].column_names\n",
        "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[text_column_name])\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=column_names,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"].map(lambda x: group_texts(x, block_size, history_size),\n",
        "                                                        batched=True, desc=f\"Grouping train in chunks of {block_size} and history {history_size}\")\n",
        "valid_dataset = tokenized_datasets[\"validation\"].map(lambda x: group_texts(x, block_size, history_size),\n",
        "                                                        batched=True, desc=f\"Grouping valid in chunks of {block_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mc-OJZ8okkvz"
      },
      "outputs": [],
      "source": [
        "train_rnd_generator = torch.Generator()\n",
        "train_rnd_generator.manual_seed(42)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn,\n",
        "                                shuffle=True, drop_last=False, generator=train_rnd_generator, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                                        collate_fn=collate_fn, shuffle=False, drop_last=True, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2rVwtGkkv0"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pnTXKDAukkv1"
      },
      "outputs": [],
      "source": [
        "gen = iter(train_dataloader)\n",
        "batch = next(gen)\n",
        "batch.pop('labels_mask')\n",
        "for k, v in batch.items():\n",
        "    batch[k] = v.to(device)\n",
        "batch['input_ids'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcdKv_JOkkv3"
      },
      "source": [
        "### Add RMT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Pm4FbNFtkkv3"
      },
      "outputs": [],
      "source": [
        "from modeling_rmt.language_modeling import MemoryCell, RecurrentWrapper\n",
        "\n",
        "cell = MemoryCell(model, num_mem_tokens=memory_size)\n",
        "model = RecurrentWrapper(cell,\n",
        "                        segment_size=block_size,\n",
        "                        max_n_segments=n_segments,\n",
        "                        )\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RIwu5F5ckkv5"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    out = model(**batch)\n",
        "    print('Success!')\n",
        "except IndexError:\n",
        "    print('Error: Input size too large!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DpEiC0hXkkv6"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "optim = AdamW(params=model.parameters(), lr=1e-03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "siMO7uwLkkv6"
      },
      "outputs": [],
      "source": [
        "#train_steps = 2000\n",
        "#eval_steps = 100\n",
        "\n",
        "train_gen = iter(train_dataloader)\n",
        "valid_gen = iter(valid_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SNzaYTxzzJyh"
      },
      "outputs": [],
      "source": [
        "run_cfg = {\n",
        "    'input_size': block_size,\n",
        "    'memory_size': memory_size,\n",
        "    'n_segments': n_segments,\n",
        "    'batch_size': batch_size,\n",
        "    'model_name': model_name,\n",
        "    'config_name': config_name,\n",
        "}\n",
        "wb_cfg.update(run_cfg)\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"RMT GPT\",\n",
        "    name='mem'+str(memory_size)+'_inlen'+str(input_size)+'_seg'+str(n_segments)+'_MultiStrS1W2'+config_name,\n",
        "    config=wb_cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JviQYL6Skkv7"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "# Initialize the progress bar\n",
        "progress_bar = tqdm.notebook.tqdm(range(train_steps), desc='Training Progress')\n",
        "\n",
        "# Create an iterator from the DataLoader\n",
        "train_iterator = iter(train_dataloader)\n",
        "\n",
        "for step in progress_bar:\n",
        "    optim.zero_grad()\n",
        "\n",
        "    try:\n",
        "        batch = next(train_iterator)\n",
        "    except StopIteration:\n",
        "        # Reset the iterator when the end of the dataset is reached\n",
        "        train_iterator = iter(train_dataloader)\n",
        "        batch = next(train_iterator)\n",
        "\n",
        "    # Move the batch to the device\n",
        "    for k, v in batch.items():\n",
        "        batch[k] = v.to(device)\n",
        "\n",
        "    out = model(**batch)\n",
        "    loss = out.loss\n",
        "\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    if step % eval_steps == 0:\n",
        "        losses.append(loss.detach().item())\n",
        "        current_loss = loss.item()\n",
        "        progress_bar.set_description(f\"Step {step}/{train_steps} - Loss: {current_loss:.4f}\")\n",
        "        wandb.log({'step': step, 'loss': current_loss})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Supx7C2ikkv9"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_base, label='Mem 0',alpha=0.5)\n",
        "plt.plot(losses, label='Mem ' + str(memory_size),alpha=0.5)\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('train loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "11Aveswvkkv9"
      },
      "outputs": [],
      "source": [
        "#loss_2seg0mem = losses\n",
        "print(memory_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BfuQ3Mm0fq6Y"
      },
      "outputs": [],
      "source": [
        "valid_losses = []\n",
        "model.eval()\n",
        "for step in tqdm.notebook.tqdm(range(eval_steps)):\n",
        "    batch = next(valid_gen)\n",
        "    for k, v in batch.items():\n",
        "        batch[k] = v.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(**batch)\n",
        "    valid_loss = out.loss\n",
        "\n",
        "    valid_losses.append(loss.detach().item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Qhs0E1ifq6Z"
      },
      "outputs": [],
      "source": [
        "print(f'Loss on {eval_steps * batch_size} validation samples: {np.mean(valid_losses)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E097CMnwoQOs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}